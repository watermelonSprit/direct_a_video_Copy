{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct-a-Video Step-by-Step Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "import imageio  # pip install imageio==2.9.0 imageio-ffmpeg==0.4.2\n",
    "\n",
    "from src.unet_3d import MyUNet3DConditionModel as UNet3DConditionModel\n",
    "from src.t2v_pipeline import TextToVideoSDPipeline, tensor2vid\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float16  # recommended to use float16 for faster inference\n",
    "seed = 1926\n",
    "\n",
    "pretrained_model_name_or_path = \"cerspense/zeroscope_v2_576w\"  # base model path\n",
    "cam_ckpt_path = \"ckpt/unet_cam_model.ckpt\"  # trained camera model path, see readme file for download link\n",
    "\n",
    "output_dir = \"outputs_test\"  # output directory (for saving results and code)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "save_project_src_files([\"src/*.py\", \"./*.py\"], os.path.join(output_dir, 'code')) # save a copy of source code\n",
    "\n",
    "num_inference_steps = 50\n",
    "f = 24  # number of frames\n",
    "h = 320 # height\n",
    "w = 512 # width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load unet\n",
    "unet_orig = UNet3DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\",torch_dtype=dtype)\n",
    "unet_config = UNet3DConditionModel.load_config(pretrained_model_name_or_path, subfolder='unet')\n",
    "unet_config['attention_type'] = 'cross_temp' # set attention type to cross_temp so that camera model can be initialized below\n",
    "unet = UNet3DConditionModel.from_config(unet_config)\n",
    "\n",
    "unet_orig_ckpt =unet_orig.state_dict()\n",
    "unet_cam_ckpt = torch.load(cam_ckpt_path, map_location='cpu')\n",
    "\n",
    "unet.load_state_dict({**unet_orig_ckpt, **unet_cam_ckpt}, strict=True)\n",
    "unet.to(dtype=dtype)\n",
    "del unet_orig, unet_cam_ckpt\n",
    "\n",
    "## Set attn processors, including temporal cross attention and spatial cross attention\n",
    "unet.set_direct_a_video_attn_processors()\n",
    "\n",
    "## load other models\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", torch_dtype=dtype)\n",
    "vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder='tokenizer', torch_dtype=dtype,)\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder='text_encoder', torch_dtype=dtype,)\n",
    "scheduler = DDIMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder='scheduler', torch_dtype=dtype )\n",
    "# scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)  # you may use other samplers other than DDIMScheduler\n",
    "\n",
    "\n",
    "## make pipeline\n",
    "pipeline = TextToVideoSDPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    ").to(device)  # .to(accelerator.device)\n",
    "\n",
    "pipeline.set_progress_bar_config(disable=False)\n",
    "\n",
    "E = pipeline.encode_pixels  # vae encoder alias, for debug use\n",
    "D = pipeline.decode_latents  # vae decoder alias, for debug use\n",
    "\n",
    "generator = None if seed is None else torch.Generator(device=device).manual_seed(seed)\n",
    "neg_prompt = \"low quality, ugly, blurry image, bad quality, low resolution, disfigured, bad anatomy, bad composition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Camera Motion Only\n",
    "\n",
    "Set prompt and camera motion parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a waterfall in a beautiful forest with fall foliage, best quality, extremely detailed, national geographic.\"\n",
    "\n",
    "cam_tx = 0.3  # x-pan ratio (-1~1), >0 for right, <0 for left\n",
    "cam_ty = -0.2  # y-pan ratio (-1~1), >0 for down, <0 for up\n",
    "cam_s = 0.85  # x-pan ratio (0.5~2), >1 for zoom in, <1 for zoom out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare cam_motion parameters\n",
    "cam_motion = torch.tensor([[[float(cam_tx), float(cam_ty), float(cam_s)]]]).to(device=device, dtype=dtype)\n",
    "\n",
    "## Run inference\n",
    "out = pipeline(\n",
    "    prompt=prompt, \n",
    "    negative_prompt=neg_prompt,\n",
    "    cam_motion=cam_motion,\n",
    "    cam_off_t=0.85,  # time step to turn off camera motion control\n",
    "    cam_cfg=True,  # enable classifier-free guidance for camera motion control\n",
    "    num_frames=f,\n",
    "    num_inference_steps=50,\n",
    "    width=w,\n",
    "    height=h,\n",
    "    # generator=[torch.Generator(device=device).manual_seed(seed)],\n",
    ").frames\n",
    "\n",
    "### Save video\n",
    "save_name = os.path.join(output_dir, f\"{prompt[:50]}_cam.mp4\")\n",
    "imageio.mimsave(save_name, out, fps=8)\n",
    "print(f\"Video saved to '{save_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Object Motion Only\n",
    "\n",
    "First let's set prompt and object bbox:\n",
    "\n",
    "**Instructions on prompt:** \n",
    "* Use * to mark the object(s) word and the background word (optional), just append * right after the word.\n",
    "    For example, \"a tiger* and a bear* walking in snow*\"\n",
    "* If an object has more than one words, use ( ) to wrap them. E.g., a (white tiger) walking in (green grassland)\"\n",
    "* The mark * and ( ) can be used together, e.g., a tiger* and a (bear) walking in (green grassland)\"\n",
    "* The marked background word (if any) should always be the last marked word, as seen with the above examples.\n",
    "\n",
    "**Instructions on bbox:** \n",
    "\n",
    "bbox describes the bounding box of objects, which depicts object's spatial-temporal motion trajectory in the video. \n",
    "\n",
    "We provide example bbox in the code below, you can run it directly. If you wish to create your own bbox, you may use our UI tool to draw boxes and save it to .npy file then load it as done in the code below. If you want to learn more details about the bbox, see instructions below:\n",
    "\n",
    "* bbox is a list of tensors, the list length is the number of objects (exclude background). If bbox list contains more than one boxes, the boxes order be consistent with the marked object words in the prompt.\n",
    "* Each tensor in bbox list should be in size of 24*4, where 24 is number of frames, 4 means [x1,y1,x2,y2], the normalized coordinates (value range 0~1) of box left-top and right-bottom corners in each frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single object motion\n",
    "prompt = \"a horse* walking in grassland*\"\n",
    "bbox = [load_bbox(\"data/box_left_to_right.npy\", f=f)] # load bbox from file\n",
    "\n",
    "# Multi objects motion\n",
    "prompt = \"an horse* and a house* in grassland*\"\n",
    "bbox1 = load_bbox(\"data/box_left_to_right.npy\", f=f)\n",
    "bbox2 = load_bbox(\"data/box_static.npy\", f=f)\n",
    "bbox = [bbox1, bbox2]\n",
    "\n",
    "## Let's visualize the object bbox\n",
    "visualize_bbox(bbox, height=h, width=w, frames=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the kwargs for object attention modulation\n",
    "# you can adjust attn_lambda to control the modulation strength \n",
    "# adjust attn_tau to control the modulation timestep (turn off when t < attn_tau)\n",
    "obj_motion_attn_kwargs = pipeline.prepare_obj_motion_attn_kwargs(prompt=prompt, bbox=bbox,\n",
    "                                                                attn_lambda=25, attn_tau=0.95,\n",
    "                                                                h=h,w=w, f=f)\n",
    "\n",
    "### Run inference\n",
    "out = pipeline(\n",
    "    prompt=prompt, \n",
    "    negative_prompt=neg_prompt,\n",
    "    # cam_motion=cam_motion,\n",
    "    # cam_off_t=0.85,  \n",
    "    # cam_cfg=True,  \n",
    "    cross_attention_kwargs=obj_motion_attn_kwargs,  #### add object motion attention control\n",
    "    num_frames=f,\n",
    "    num_inference_steps=50,\n",
    "    width=w,\n",
    "    height=h,\n",
    "    # generator=[torch.Generator(device=device).manual_seed(seed)],\n",
    ").frames\n",
    "\n",
    "### Save video\n",
    "save_name = os.path.join(output_dir, f\"{prompt[:50].replace('*','')}_obj.mp4\")\n",
    "imageio.mimsave(save_name, out, fps=8)\n",
    "print(f\"Video saved to '{save_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference: Camera + Object Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Camera motion\n",
    "cam_tx = 0.3  # x-pan ratio (-1~1), >0 for right, <0 for left\n",
    "cam_ty = 0  # y-pan ratio (-1~1), >0 for down, <0 for up\n",
    "cam_s = 1.2  # x-pan ratio (0.5~2), >1 for zoom in, <1 for zoom out\n",
    "\n",
    "\n",
    "## Single object motion\n",
    "prompt = \"a zebra* walking along the river*\"\n",
    "bbox = [load_bbox(\"data/box_left_to_right.npy\", f=f)] # load bbox from file\n",
    "\n",
    "## Multi objects motion\n",
    "# prompt = \"a tiger* and a bear* walking in grass*\"\n",
    "# bbox1 = load_bbox(\"./box/11.npy\", f=f)\n",
    "# bbox2 = load_bbox(\"./box/22.npy\", f=f)\n",
    "# bbox = [bbox1, bbox2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare cam_motion parameters\n",
    "cam_motion = torch.tensor([[[float(cam_tx), float(cam_ty), float(cam_s)]]]).to(device=device, dtype=dtype)\n",
    "\n",
    "## Prepare the kwargs for object motion\n",
    "obj_motion_attn_kwargs = pipeline.prepare_obj_motion_attn_kwargs(prompt=prompt, bbox=bbox,\n",
    "                                                                attn_lambda=25, attn_tau=0.95,\n",
    "                                                                h=h,w=w, f=f)\n",
    "## Run inference\n",
    "out = pipeline(\n",
    "    prompt=prompt, \n",
    "    negative_prompt=neg_prompt,\n",
    "\n",
    "    cam_motion=cam_motion,\n",
    "    cam_off_t=0.85,  # time step to turn off camera motion control\n",
    "    cam_cfg=True,  # enable classifier-free guidance for camera motion control\n",
    "\n",
    "    cross_attention_kwargs=obj_motion_attn_kwargs,  # add object motion attention control\n",
    "\n",
    "    num_frames=f,\n",
    "    num_inference_steps=50,\n",
    "    width=w,\n",
    "    height=h,\n",
    "    # generator=[torch.Generator(device=device).manual_seed(seed)],\n",
    ").frames\n",
    "\n",
    "### Save video\n",
    "save_name = os.path.join(output_dir, f\"{prompt[:50].replace('*','')}_cam_obj.mp4\")\n",
    "imageio.mimsave(save_name, out, fps=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
